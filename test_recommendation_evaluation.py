#!/usr/bin/env python3
"""
Test script for academically rigorous recommendation evaluation.

This script demonstrates:
1. Proper train/validation/test data splits
2. Multiple baseline recommenders
3. Real recommendation metrics (Precision@K, Recall@K, NDCG@K)
4. Cluster-based recommendation with actual model training
5. Comparison against baselines
"""

import asyncio
import logging
import sys
from pathlib import Path

# Add project root to path
sys.path.append(str(Path(__file__).parent))
sys.path.append(str(Path(__file__).parent / "src"))

from vulcan.core.config_manager import ConfigManager
from vulcan.data.goodreads_loader import GoodreadsDataLoader
from vulcan.evaluation.recommendation_evaluator import RecommendationEvaluator
from vulcan.features import FeatureExecutor
from vulcan.types import (
    FeatureDefinition,
    FeatureSet,
    FeatureType,
    MCTSAction,
)

# Setup logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


async def test_recommendation_evaluation():
    """Test the academically rigorous recommendation evaluation system."""

    # Load configuration
    config_manager = ConfigManager()
    config = config_manager.config
    logger.info("Configuration loaded")

    # Load data with proper splits
    loader = GoodreadsDataLoader(
        db_path="data/goodreads.db",
        splits_dir="data/splits",
        outer_fold=1,  # Use first fold for testing
        inner_fold=1,
    )
    data_context = loader.get_data_context(
        sample_size=50000  # Reasonable sample for testing
    )
    logger.info(
        f"Data loaded: {data_context.n_users} users, {data_context.n_items} items"
    )

    # Initialize evaluator and feature executor
    evaluator = RecommendationEvaluator(config)
    await evaluator.initialize()
    logger.info("Recommendation evaluator initialized")

    executor = FeatureExecutor(config)
    await executor.initialize()
    logger.info("Feature executor initialized")

    # Create test features (these would be generated by VULCAN in practice)
    test_features = [
        FeatureDefinition(
            name="user_avg_rating",
            feature_type=FeatureType.CODE_BASED,
            description="Average rating per user",
            code="""
# Calculate average rating per user
result = df.groupby('user_id')['rating'].mean()
result.name = 'user_avg_rating'
            """,
            dependencies=["user_id", "rating"],
            computational_cost=1.0,
        ),
        FeatureDefinition(
            name="user_rating_count",
            feature_type=FeatureType.CODE_BASED,
            description="Number of ratings per user",
            code="""
# Count ratings per user
result = df.groupby('user_id')['rating'].count()
result.name = 'user_rating_count'
            """,
            dependencies=["user_id", "rating"],
            computational_cost=1.0,
        ),
        FeatureDefinition(
            name="user_rating_variance",
            feature_type=FeatureType.CODE_BASED,
            description="Rating variance per user",
            code="""
# Calculate rating variance per user
result = df.groupby('user_id')['rating'].var().fillna(0)
result.name = 'user_rating_variance'
            """,
            dependencies=["user_id", "rating"],
            computational_cost=1.5,
        ),
    ]

    # Execute features using the actual FeatureExecutor
    logger.info("Executing features on training data...")

    # Execute features on the full dataset (will use train split internally)
    feature_results = await executor.execute_feature_set(
        features=test_features,
        data_context=data_context,
        target_split="train",  # Use train split for feature engineering
    )

    logger.info(
        f"Feature execution complete. Results for {len(feature_results)} features"
    )

    # Create feature set
    feature_set = FeatureSet(features=test_features, action_taken=MCTSAction.ADD)

    # Evaluate features with real recommendation metrics
    logger.info("\n" + "=" * 60)
    logger.info("STARTING ACADEMICALLY RIGOROUS EVALUATION")
    logger.info("=" * 60 + "\n")

    evaluation = await evaluator.evaluate_feature_set(
        feature_set=feature_set,
        feature_results=feature_results,  # Pass the actual execution results
        data_context=data_context,
        iteration=1,
    )

    # Display comprehensive results
    print("\n" + "=" * 80)
    print("RECOMMENDATION EVALUATION RESULTS")
    print("=" * 80)

    print(f"\nüìä Overall Score: {evaluation.overall_score:.4f}")

    print("\nüéØ Recommendation Metrics (on held-out TEST set):")
    print(f"  Precision@10: {evaluation.metrics.precision_at_10:.4f}")
    print(f"  Recall@10: {evaluation.metrics.recall_at_10:.4f}")
    print(f"  NDCG@10: {evaluation.metrics.ndcg_at_10:.4f}")

    print("\nüìà Performance Comparison:")
    if hasattr(evaluator, "baseline_scores"):
        best_baseline = max(evaluator.baseline_scores.items(), key=lambda x: x[1])
        print(f"  Best Baseline ({best_baseline[0]}): {best_baseline[1]:.4f}")
        print(
            f"  Global with Features: {evaluation.metrics.global_with_features_p10:.4f} "
            f"(+{evaluation.metrics.improvement_global:.1%})"
        )
        print(
            f"  Cluster-based with Features: {evaluation.metrics.precision_at_10:.4f} "
            f"(+{evaluation.metrics.improvement_over_baseline:.1%})"
        )

    print("\nüîç Clustering Analysis:")
    print(f"  Optimal Clusters Found: {evaluation.metrics.num_clusters}")
    print(f"  Cluster Coverage: {evaluation.metrics.cluster_coverage:.2%}")
    print(f"  Silhouette Score: {evaluation.metrics.silhouette_score:.4f}")

    print(f"\n‚è±Ô∏è  Evaluation Time: {evaluation.evaluation_time:.2f} seconds")

    # Show baseline comparison
    if hasattr(evaluator, "baseline_scores"):
        print("\nüìä Baseline Comparison:")
        print("-" * 40)
        print(f"{'Baseline':<15} {'Precision@10':>12}")
        print("-" * 40)
        for name, score in sorted(evaluator.baseline_scores.items()):
            print(f"{name:<15} {score:>12.4f}")
        print("-" * 40)

        # Highlight best baseline
        best_baseline = max(evaluator.baseline_scores.items(), key=lambda x: x[1])
        print(f"\nBest Baseline: {best_baseline[0]} ({best_baseline[1]:.4f})")
        print(f"Our Method: Cluster-based ({evaluation.metrics.precision_at_10:.4f})")
        print(f"Improvement: {evaluation.metrics.improvement_over_baseline:.2%}")

    print("\n" + "=" * 80)

    # Test with different feature combinations
    print("\n\nTesting with single feature (ablation study)...")
    single_feature_set = FeatureSet(
        features=[test_features[0]],  # Just average rating
        action_taken=MCTSAction.ADD,
    )

    # Execute just the single feature
    single_feature_results = await executor.execute_feature_set(
        features=[test_features[0]],
        data_context=data_context,
        target_split="train",
    )

    single_evaluation = await evaluator.evaluate_feature_set(
        feature_set=single_feature_set,
        feature_results=single_feature_results,
        data_context=data_context,
        iteration=2,
    )

    print("\nSingle Feature Results:")
    print(f"  Overall Score: {single_evaluation.overall_score:.4f}")
    print(f"  Precision@10: {single_evaluation.metrics.precision_at_10:.4f}")
    print(f"  Optimal Clusters: {single_evaluation.metrics.num_clusters}")
    print(
        f"  Improvement over baseline: {single_evaluation.metrics.improvement_over_baseline:.2%}"
    )

    # Summary for academic paper
    print("\n\n" + "=" * 80)
    print("SUMMARY FOR ACADEMIC PAPER")
    print("=" * 80)
    print(f"""
1. Data Splits:
   - Train: Feature engineering and cluster model training
   - Validation: Hyperparameter tuning (number of clusters)
   - Test: Final evaluation (never seen during training)

2. Baselines Evaluated:
   - Random recommendations
   - Popularity-based
   - User-based Collaborative Filtering (KNN)
   - Item-based Collaborative Filtering (KNN)
   - Matrix Factorization (SVD)
   - LightFM (Factorization Machines)
   - BPR (Bayesian Personalized Ranking)

3. Our Method:
   - Cluster users based on VULCAN-generated features
   - Train separate recommender per cluster
   - Evaluate on held-out test set

4. Key Results:
   - Best Baseline: {best_baseline[0]} with Precision@10 = {best_baseline[1]:.4f}
   - Our Method: Precision@10 = {evaluation.metrics.precision_at_10:.4f}
   - Improvement: {evaluation.metrics.improvement_over_baseline:.2%}
   - Statistical significance: Would compute with bootstrap resampling

5. Ablation Study:
   - Single feature: {single_evaluation.metrics.precision_at_10:.4f}
   - All features: {evaluation.metrics.precision_at_10:.4f}
   - Shows importance of feature combination
    """)
    print("=" * 80)

    # Cleanup
    await executor.cleanup()

    return evaluation


if __name__ == "__main__":
    asyncio.run(test_recommendation_evaluation())
