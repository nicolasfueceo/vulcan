

üîª Final Introduction Structure: Funnel + Integrated Literature Review

‚∏ª

1. Recommender Systems: A Cornerstone of the Modern Web

Purpose: Set context for the field, show real-world relevance, and trace foundational progress.

	‚Ä¢	Define recommender systems and their ubiquity (Netflix, Spotify, Amazon).
	‚Ä¢	Trace evolution:
Collaborative Filtering ‚Üí Matrix Factorization (MF) ‚Üí Neural Models.
	‚Ä¢	Cite key works:
	‚Ä¢	GroupLens (Resnick et al., 1994) ‚Äì origin of CF.
	‚Ä¢	Amazon Item-to-Item CF (Linden et al., 2003) ‚Äì scalable recommender.
	‚Ä¢	Netflix Prize + SVD++ (Koren et al., 2009) ‚Äì MF dominance.
	‚Ä¢	BERT4Rec (Sun et al., 2019) ‚Äì sequential modeling.
	‚Ä¢	Conclude: Model sophistication is no longer the bottleneck. Feature quality is.

‚∏ª

2. Feature Engineering: The Forgotten Bottleneck

Purpose: Shift the lens from models to features, building the problem case for your thesis.

	‚Ä¢	Define feature engineering in ML: turning raw data into informative signals.
	‚Ä¢	Challenges in recommenders:
	‚Ä¢	Sparse data.
	‚Ä¢	Cold start problems.
	‚Ä¢	Dependence on expert-crafted features.
	‚Ä¢	Manual FE: slow, limited in scope, hard to iterate.
	‚Ä¢	Introduce Automated Feature Engineering (AutoFE):
	‚Ä¢	Examples: Featuretools, Deep Feature Synthesis (Kanter & Veeramachaneni, 2015).
	‚Ä¢	Problems: lack of domain awareness, black-box behavior, no strategic reasoning.
	‚Ä¢	Emphasize: Most AutoFE is combinatorial, not conceptual.

‚∏ª

3. LLMs: A New Tool for Feature Intelligence

Purpose: Introduce LLMs as an enabling breakthrough for feature engineering.

	‚Ä¢	LLMs offer:
	‚Ä¢	Natural language reasoning.
	‚Ä¢	General world knowledge.
	‚Ä¢	Ability to express and refine ideas programmatically.
	‚Ä¢	Recent work:
	‚Ä¢	KAR (Zhang et al., 2022): Reasoning-driven augmentation for recommender systems.
	‚Ä¢	KALM4Rec (Huang et al., 2023): Use of keywords + LLM for fine-tuned recall.
	‚Ä¢	AutoGPT-FE, FEGPT: Zero-shot or chain-of-thought for tabular tasks.
	‚Ä¢	Limitations of prior work:
	‚Ä¢	Often use LLM as one-off generator.
	‚Ä¢	No structured collaboration.
	‚Ä¢	No long-horizon memory or meta-learning.
	‚Ä¢	Transition: What if we treated LLMs not as tools, but as agents in a team?

‚∏ª

4. Multi-Agent Systems: A Collaborative Intelligence Framework

Purpose: Introduce the paradigm shift ‚Äî from single-agent prompting to structured reasoning via teams.

	‚Ä¢	Define Multi-Agent Systems (MAS) in AI:
	‚Ä¢	Systems of agents with individual roles, goals, and communication protocols.
	‚Ä¢	Used in robotics, games, distributed problem-solving.
	‚Ä¢	Benefits:
	‚Ä¢	Specialization.
	‚Ä¢	Division of labor.
	‚Ä¢	Iterative critique.
	‚Ä¢	Literature to cite:
	‚Ä¢	AgentGPT, AutoGen, CAMEL: Foundation of LLM collaboration.
	‚Ä¢	AgentCF (Zhao et al., 2024): Multi-agent recsys architecture.
	‚Ä¢	LLM4TS, LLM Agents for Data Science Tasks.
	‚Ä¢	Your thesis hypothesis:
‚ÄúA structured, multi-agent LLM system can discover better, more novel, and more interpretable features than monolithic approaches.‚Äù

‚∏ª

5. Formal Problem Setup: A Bilevel Optimization View

Purpose: Introduce a mathematical/engineering framing that defines the optimization your system performs.

üìå We already outlined this in our previous message. This is the ideal place to drop it in ‚Äî complete with definitions, assumptions, and the bilevel equation.

‚∏ª

6. VULCAN: An Agentic System for Strategic Feature Discovery

Purpose: Describe the system you built ‚Äî only high-level, saving implementation for later.

	‚Ä¢	Present the GroupChat architecture:
	‚Ä¢	Insight Discovery Team: EDA + pattern search.
	‚Ä¢	Strategy Team: Hypothesis vetting + feature ideation.
	‚Ä¢	Emphasize modularity and roles.
	‚Ä¢	Downstream: Feature realization ‚Üí Optimization ‚Üí Evaluation.
	‚Ä¢	Novel components:
	‚Ä¢	Bilevel Optimization with multi-objective function.
	‚Ä¢	Explicit clustering for structure-aware feature tuning.
	‚Ä¢	Reflection and meta-learning across pipeline runs.

‚∏ª

7. Research Questions

Purpose: Ground your thesis in research methodology.

	‚Ä¢	RQ1: Does a collaborative multi-agent LLM system produce higher-performing features for recommendation?
	‚Ä¢	RQ2: What is the ablation impact of the GroupChat structure vs linear pipeline?
	‚Ä¢	RQ3: How do VULCAN-generated features score in interpretability, novelty, and diversity?

‚∏ª

8. Thesis Roadmap

Purpose: Help the reader navigate what‚Äôs next.

Example:

	‚Ä¢	Chapter 2: Related Work (embedded throughout intro).
	‚Ä¢	Chapter 3: Methodology ‚Äî design and architecture of VULCAN.
	‚Ä¢	Chapter 4: Experimental Setup ‚Äî datasets, baselines, metrics.
	‚Ä¢	Chapter 5: Results & Discussion ‚Äî evaluation and ablations.
	‚Ä¢	Chapter 6: Conclusion ‚Äî implications, limitations, future work.

‚∏ª

Would you like this now written into full prose, or do you want to iterate further on the outline or individual sections first?



FIGURES:
- High level conceptual overview of the pipeline

