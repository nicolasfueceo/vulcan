\doublespacing

\chapter{Introduction}
\label{ch:intro}

\section{The Centrality of Recommendation in the Digital Economy}
\label{sec:intro_motivation}

Recommender systems have evolved from a niche academic interest into fundamental infrastructure of the digital economy. They mediate a substantial proportion of user interactions with information, shaping content consumption on platforms such as Netflix and YouTube, influencing purchasing decisions on Amazon, and curating cultural exposure on Spotify. The economic and societal impact is considerable; however, despite decades of advancement, these systems grapple with foundational challenges that limit their efficacy and trustworthiness. A primary issue is the \textit{cold-start problem}, which arises when the system has insufficient interaction data for new users or items, rendering it incapable of producing accurate, personalised recommendations~\cite{Schein2002ColdStart}. This dependency on historical data reveals a critical limitation of many classical architectures. Concurrently, a growing demand for algorithmic transparency has highlighted the opacity of many systems. Users are frequently presented with recommendations without a clear rationale, eroding trust and preventing meaningful user control—a deficiency termed a lack of \textit{scrutability}~\cite{Tintarev2011Explainable}. These persistent challenges underscore the need for new paradigms that move beyond simple pattern matching towards more robust, transparent, and data-efficient reasoning.

\section{A Multi-Paradigm Evolution and the Feature Engineering Bottleneck}
\label{sec:intro_evolution}

The history of recommender systems is best understood as a series of paradigm shifts, each developed to address the shortcomings of its predecessors. The field's inception can be traced to early \textit{Collaborative Filtering} (CF) systems, which operated on the intuitive principle that users who have agreed in the past will agree in the future~\cite{Resnick1994GroupLens}. While pioneering, these memory-based methods struggled with the scalability and data sparsity inherent in large-scale systems. A significant breakthrough came with the popularisation of \textit{Matrix Factorisation} (MF) techniques during the Netflix Prize competition~\cite{Koren2009MatrixFactorization}. By decomposing the sparse user-item interaction matrix into low-dimensional latent factor representations for users and items, MF models could generalise far more effectively and capture nuanced preferences. This marked a shift towards model-based approaches.

The subsequent rise of deep learning introduced a third paradigm, enabling the modelling of complex, non-linear relationships within the data. Architectures such as \textit{Neural Collaborative Filtering}~\cite{He2017NCF} and Google's two-tower \textit{Wide \& Deep} networks~\cite{Cheng2016WideDeep} demonstrated superior performance by learning intricate feature interactions automatically. However, extensive research has shown that the performance gains from increasingly complex model architectures are subject to diminishing returns. The critical insight, first articulated by Koren et al. and now widely accepted, is that the ultimate performance of any recommender system is fundamentally constrained by the quality of its input features~\cite{Koren2009MatrixFactorization}. This has reframed the central challenge in the field, shifting focus from model architecture design to the \textit{feature engineering bottleneck}.

\section{Automated Feature Engineering: Promise and Unfulfilled Potential}
\label{sec:intro_autofe}

Feature engineering—the process of transforming raw data into informative representations—has traditionally been a manual, labor-intensive task reliant on domain expertise and intuition. In response to this bottleneck, the field of \textit{Automated Feature Engineering} (AutoFE) emerged, aiming to systematize and accelerate feature discovery. Seminal systems like Deep Feature Synthesis (DFS) approached this problem through combinatorial exploration, automatically applying a set of mathematical primitives to generate thousands of candidate features from relational data~\cite{Kanter2015DeepFeatureSynthesis}.

While powerful, these first-generation AutoFE tools suffer from a critical limitation: they operate without semantic understanding or strategic reasoning. The combinatorial explosion of features they produce is often dominated by irrelevant or redundant candidates, requiring extensive downstream filtering. These systems are fundamentally syntactic, unable to distinguish between a feature that is mathematically valid and one that is semantically meaningful for the recommendation task at hand. This lack of domain-aware, hypothesis-driven exploration represents a significant gap, preventing these tools from replicating the strategic intelligence of an expert data scientist.

\section{The Advent of Large Language Models as Reasoning Engines}
\label{sec:intro_llms}

\textbf{Pipeline Integration and Empirical Results.} Large Language Models (LLMs) have redefined the technical boundaries of recommender systems, with empirical evidence for their impact at every stage of the pipeline. In feature engineering, LLMs can be prompted to generate candidate features that encode domain knowledge, user intent, and semantic context, outperforming combinatorial methods in both diversity and downstream model performance~\cite{Zou2025FEBP, Wang2023LLMAgentsSurvey}. For representation learning, LLMs enable the construction of user and item embeddings directly from text, allowing for the incorporation of world knowledge and context-specific information~\cite{Qiu2021UBERT, Wang2024RecMind}. In data augmentation, LLM-powered synthetic user agents have been shown to mitigate data sparsity, especially in cold-start and conversational settings~\cite{Mysore2023NarrativeDriven, Wang2024RecMind}.

\textbf{Architectural Paradigms.} The literature identifies four principal architectural paradigms, determined by two axes: (1) whether the LLM is fine-tuned or "frozen" (used zero-shot), and (2) whether it serves as the core recommendation engine or as an augmentation to classical models~\cite{Lin2024Survey, Xu2024Prompting}. Fine-tuned LLMs as core engines (Quadrant IV) have achieved state-of-the-art results in sequential and conversational recommendation, especially when coupled with retrieval modules for re-ranking~\cite{Cao2024Aligning}. However, these approaches are computationally intensive and raise privacy concerns. Frozen LLMs as augmentations (Quadrant II) offer scalability and compatibility with legacy systems, but empirical studies report diminished explainability and lower gains in complex, domain-specific tasks~\cite{Qiu2021UBERT, Lin2024Survey}. A critical empirical finding is the necessity of infusing collaborative information—either by fine-tuning or by hybridising with collaborative filtering signals—to surpass the performance boundary observed in purely language-based models~\cite{Lin2024Survey, Zhang2023AgentCF}.

\textbf{Agentic and Multi-Agent Systems.} Recent research, including our own, demonstrates that single-agent LLMs are fundamentally limited in their capacity for hypothesis-driven exploration, self-critique, and iterative refinement~\cite{Zhang2023AgentCF, Wang2023LLMAgentsSurvey}. Agentic frameworks, such as VULCAN, decompose the feature engineering and evaluation workflow into specialised LLM-powered agents responsible for insight discovery, strategy formation, code implementation, and critical evaluation. This multi-agent paradigm has been shown to improve both the diversity and quality of engineered features, as well as the robustness of model validation, by enabling collaborative, iterative, and adversarial interactions between agents~\cite{Zhang2023AgentCF, Wang2024RecMind}.

\textbf{Technical Challenges and Open Questions.} Despite these advances, LLM-based recommenders face unresolved challenges. Bias and fairness remain open problems, with studies documenting demographic and cognitive biases in LLM-generated recommendations~\cite{Deldjoo2024Biases, Lyu2024CognitiveBias}. Data privacy and prompt injection attacks are active areas of concern, necessitating new protocols for secure deployment~\cite{Yi2024PromptInjection}. Empirical benchmarking is still evolving, with the community converging on metrics such as NDCG, precision@k, and ablation studies to quantify the contribution of each architectural component~\cite{Wang2023LLMAgentsSurvey, Lin2024Survey}. The generation and use of synthetic conversational data, as pioneered in this project, is emerging as a key enabler for robust evaluation and transferability.

In summary, LLMs have shifted the field from static, pattern-matching systems to dynamic, reasoning-driven architectures. The agentic, multi-agent approach operationalised in this thesis represents the current empirical frontier, but its full potential—and limitations—remain contingent on ongoing advances in bias mitigation, privacy, and benchmarking.
\section{Problem Statement: Towards Collaborative, Agentic Intelligence}
\label{sec:intro_problem}

The preceding analysis reveals a clear research gap. The field has progressed from manual feature engineering to combinatorial automation and, recently, to semantic generation with single LLMs. Yet, no existing framework effectively integrates strategic, hypothesis-driven reasoning, collaborative problem-solving, and iterative validation into a single, cohesive system for feature engineering. The central problem this thesis addresses is therefore formally stated as:

\blockquote{\itshape
How can a collaborative multi-agent system be designed to leverage the reasoning capabilities of Large Language Models to autonomously discover, implement, and validate high-quality, interpretable features for recommender systems?}

At its theoretical core, this problem can be framed as a \textit{bilevel optimization} task [CITE: Colson et al., 2007]. The overall goal is to find a set of features that maximizes recommendation quality. This constitutes the \textit{outer loop} of the optimization. However, the quality of any given feature set can only be evaluated after a recommendation model has been trained using those features—the \textit{inner loop}. The performance of the inner loop is directly dependent on the decisions made in the outer loop. Formally, we seek to optimize a vector of feature-engineering hyperparameters, $\theta$, to minimize a complex objective function $J(\theta)$:

\begin{equation}
    \theta^* = \arg\min_{\theta} J(L(M(D_{train}, \theta)))
\end{equation}

where $M$ is the recommender model, $D_{train}$ is the training data, $L$ is the model's loss on a validation set, and $J$ is the outer-loop objective function that may include terms for accuracy, feature complexity, and interpretability [cite: context.md]. Existing AutoFE methods attempt to solve this by brute-force search over $\theta$, whereas this thesis proposes to solve it through an intelligent, agentic search.

\section{Contributions and Thesis Outline}
\label{sec:intro_contributions}

To address the problem statement, this thesis presents VULCAN, a novel multi-agent framework for automated feature engineering. The primary contributions of this work are fourfold:

\begin{enumerate}
    \item \textbf{A Novel Multi-Agent Architecture for Feature Engineering.} We design, implement, and evaluate VULCAN, a framework that operationalizes the data science workflow by decomposing it into distinct, collaborative agent teams responsible for insight discovery, strategy formation, feature implementation, and optimization.
    \item \textbf{Agentic Algorithms for Hypothesis-Driven Feature Synthesis.} We develop novel prompting strategies and a self-correction loop that enables an LLM-powered agent to translate high-level hypotheses into robust, validated, and parameterized feature-generation code.
    \item \textbf{A Rigorous Empirical Evaluation.} We conduct a comprehensive set of experiments benchmarking the features generated by VULCAN against those from manual engineering and state-of-the-art automated baselines, including Featuretools, on multiple real-world datasets.
    \item \textbf{An Open-Source Toolkit for Reproducible Research.} We release the complete source code for the VULCAN framework, experimental pipelines, and analysis scripts to ensure reproducibility and facilitate future research in agentic data science.
\end{enumerate}

The remainder of this thesis is structured to detail these contributions. \textbf{Chapter 2} describes the methodology, detailing the VULCAN system architecture, the roles of each agent, and the bilevel optimization objective. \textbf{Chapter 3} outlines the experimental setup, including the datasets, baseline models, and evaluation metrics used for our comparative analysis. \textbf{Chapter 4} presents and discusses the empirical results, including ablation studies that analyze the impact of key system components. Finally, \textbf{Chapter 5} concludes by summarizing the findings, acknowledging the limitations of this work, and proposing directions for future research.
