{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goodreads Fantasy & Paranormal Dataset - Advanced EDA\n",
    "\n",
    "This notebook implements Phase 1 of exploratory data analysis and generates innovative features for the Goodreads Fantasy & Paranormal dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import gzip\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"text.usetex\": True,\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": [\"Computer Modern Roman\"],\n",
    "        \"text.latex.preamble\": r\"\\usepackage{amsmath} \\usepackage{amssymb}\",\n",
    "        \"axes.labelsize\": 12,\n",
    "        \"font.size\": 11,\n",
    "        \"legend.fontsize\": 10,\n",
    "        \"xtick.labelsize\": 10,\n",
    "        \"ytick.labelsize\": 10,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading Functions\n",
    "\n",
    "Functions to load and process the Goodreads dataset files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_gz(file_path, data_type):\n",
    "    \"\"\"Load data from a gzipped JSON file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the gzipped JSON file.\n",
    "        data_type (str): Type of data being loaded (for logging).\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the loaded data.\n",
    "    \"\"\"\n",
    "    print(f\"Loading {data_type} from {file_path}\")\n",
    "    \n",
    "    # Read gzipped JSON file line by line\n",
    "    data = []\n",
    "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=f\"Loading {data_type}\"):\n",
    "            try:\n",
    "                item = json.loads(line)\n",
    "                data.append(item)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Warning: Error decoding JSON line in {file_path}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    print(f\"Loaded {len(df)} {data_type}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your data paths here\n",
    "DATA_DIR = \"data/interim/goodreads/\"\n",
    "INTERACTIONS_PATH = os.path.join(DATA_DIR, \"goodreads_interactions_fantasy_paranormal.json.gz\")\n",
    "BOOKS_PATH = os.path.join(DATA_DIR, \"goodreads_books_fantasy_paranormal.json.gz\")\n",
    "REVIEWS_PATH = os.path.join(DATA_DIR, \"goodreads_reviews_fantasy_paranormal.json.gz\")\n",
    "\n",
    "# Create output directory for results\n",
    "OUTPUT_DIR = \"./eda_results\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, \"plots\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, \"features\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Data\n",
    "\n",
    "Load the Goodreads dataset files. You can adjust the sample size or comment out this cell if you've already loaded the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data (adjust sample sizes as needed)\n",
    "# For initial exploration, we'll use smaller samples\n",
    "# Comment out if you've already loaded the data\n",
    "\n",
    "# Uncomment and run this cell to load samples of the data\n",
    "# interactions_df = load_json_gz(INTERACTIONS_PATH, \"interactions\")[:50000]\n",
    "# books_df = load_json_gz(BOOKS_PATH, \"books\")[:10000]\n",
    "# reviews_df = load_json_gz(REVIEWS_PATH, \"reviews\")[:50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Integrity Analysis\n",
    "\n",
    "Analyze dataset integrity, including missing values and inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_integrity_analysis(books_df, reviews_df=None, interactions_df=None):\n",
    "    \"\"\"Analyze dataset integrity, including missing values and inconsistencies.\"\"\"\n",
    "    print(\"Performing dataset integrity analysis\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Check for missing values\n",
    "    books_missing = books_df.isnull().sum() / len(books_df) * 100\n",
    "    results[\"books_missing_percentages\"] = books_missing.to_dict()\n",
    "\n",
    "    # Create a bar plot of missing values in books\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    books_missing.sort_values(ascending=False).plot(kind=\"bar\")\n",
    "    plt.title(r\"\\textbf{Missing Values in Books Dataset (\\%)}\")\n",
    "    plt.xlabel(r\"\\textbf{Column}\")\n",
    "    plt.ylabel(r\"\\textbf{Percentage Missing}\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if reviews_df is not None:\n",
    "        reviews_missing = reviews_df.isnull().sum() / len(reviews_df) * 100\n",
    "        results[\"reviews_missing_percentages\"] = reviews_missing.to_dict()\n",
    "\n",
    "        # Create a bar plot of missing values in reviews\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        reviews_missing.sort_values(ascending=False).plot(kind=\"bar\")\n",
    "        plt.title(r\"\\textbf{Missing Values in Reviews Dataset (\\%)}\")\n",
    "        plt.xlabel(r\"\\textbf{Column}\")\n",
    "        plt.ylabel(r\"\\textbf{Percentage Missing}\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.tight_layout()\n",
    "\n",
    "    if interactions_df is not None:\n",
    "        interactions_missing = (\n",
    "            interactions_df.isnull().sum() / len(interactions_df) * 100\n",
    "        )\n",
    "        results[\"interactions_missing_percentages\"] = interactions_missing.to_dict()\n",
    "\n",
    "        # Create a bar plot of missing values in interactions\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        interactions_missing.sort_values(ascending=False).plot(kind=\"bar\")\n",
    "        plt.title(r\"\\textbf{Missing Values in Interactions Dataset (\\%)}\")\n",
    "        plt.xlabel(r\"\\textbf{Column}\")\n",
    "        plt.ylabel(r\"\\textbf{Percentage Missing}\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.tight_layout()\n",
    "\n",
    "    # Rest of function remains the same\n",
    "    # Identify inconsistencies in book metadata\n",
    "    results[\"duplicate_books\"] = books_df.duplicated(subset=[\"book_id\"]).sum()\n",
    "    results[\"books_with_missing_titles\"] = books_df[books_df[\"title\"].isnull()].shape[0]\n",
    "    results[\"books_with_missing_authors\"] = books_df[\n",
    "        books_df[\"authors\"].isnull()\n",
    "    ].shape[0]\n",
    "\n",
    "    # Validate timestamps if available\n",
    "    if reviews_df is not None and \"date_added\" in reviews_df.columns:\n",
    "        reviews_df[\"date_added\"] = pd.to_datetime(\n",
    "            reviews_df[\"date_added\"], errors=\"coerce\"\n",
    "        )\n",
    "        current_date = datetime.now()\n",
    "        results[\"invalid_timestamps\"] = reviews_df[\n",
    "            reviews_df[\"date_added\"] > current_date\n",
    "        ].shape[0]\n",
    "        results[\"reviews_without_timestamps\"] = reviews_df[\n",
    "            reviews_df[\"date_added\"].isnull()\n",
    "        ].shape[0]\n",
    "\n",
    "    # Examine rating distributions\n",
    "    if reviews_df is not None and \"rating\" in reviews_df.columns:\n",
    "        rating_distribution = (\n",
    "            reviews_df[\"rating\"].value_counts(normalize=True).sort_index()\n",
    "        )\n",
    "        results[\"rating_distribution\"] = rating_distribution.to_dict()\n",
    "\n",
    "        # Check for anomalies in ratings\n",
    "        results[\"invalid_ratings\"] = reviews_df[\n",
    "            ~reviews_df[\"rating\"].between(1, 5)\n",
    "        ].shape[0]\n",
    "\n",
    "        # Plot rating distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.countplot(x=\"rating\", data=reviews_df)\n",
    "        plt.title(r\"\\textbf{Rating Distribution}\")\n",
    "        plt.xlabel(r\"$\\textbf{Rating} \\in [1,5]$\")\n",
    "        plt.ylabel(r\"\\textbf{Count}\")\n",
    "\n",
    "    # Print summary of findings\n",
    "    print(\"\\nDataset Integrity Analysis Summary:\")\n",
    "    print(f\"Books with duplicate IDs: {results['duplicate_books']}\")\n",
    "    print(f\"Books with missing titles: {results['books_with_missing_titles']}\")\n",
    "    print(f\"Books with missing authors: {results['books_with_missing_authors']}\")\n",
    "\n",
    "    if \"invalid_timestamps\" in results:\n",
    "        print(f\"Reviews with invalid timestamps: {results['invalid_timestamps']}\")\n",
    "    if \"invalid_ratings\" in results:\n",
    "        print(f\"Reviews with invalid ratings: {results['invalid_ratings']}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run dataset integrity analysis\n",
    "# integrity_results = dataset_integrity_analysis(books_df, reviews_df, interactions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. User Behavior Profiling\n",
    "\n",
    "Profile user behavior, including activity levels and rating patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_integrity_analysis(books_df, reviews_df=None, interactions_df=None):\n",
    "    \"\"\"Analyze dataset integrity, including missing values and inconsistencies.\"\"\"\n",
    "    print(\"Performing dataset integrity analysis\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Check for missing values\n",
    "    books_missing = books_df.isnull().sum() / len(books_df) * 100\n",
    "    results[\"books_missing_percentages\"] = books_missing.to_dict()\n",
    "\n",
    "    # Create a bar plot of missing values in books\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    books_missing.sort_values(ascending=False).plot(kind=\"bar\")\n",
    "    plt.title(r\"\\textbf{Missing Values in Books Dataset (\\%)}\")\n",
    "    plt.xlabel(r\"\\textbf{Column}\")\n",
    "    plt.ylabel(r\"\\textbf{Percentage Missing}\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if reviews_df is not None:\n",
    "        reviews_missing = reviews_df.isnull().sum() / len(reviews_df) * 100\n",
    "        results[\"reviews_missing_percentages\"] = reviews_missing.to_dict()\n",
    "\n",
    "        # Create a bar plot of missing values in reviews\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        reviews_missing.sort_values(ascending=False).plot(kind=\"bar\")\n",
    "        plt.title(r\"\\textbf{Missing Values in Reviews Dataset (\\%)}\")\n",
    "        plt.xlabel(r\"\\textbf{Column}\")\n",
    "        plt.ylabel(r\"\\textbf{Percentage Missing}\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.tight_layout()\n",
    "\n",
    "    if interactions_df is not None:\n",
    "        interactions_missing = (\n",
    "            interactions_df.isnull().sum() / len(interactions_df) * 100\n",
    "        )\n",
    "        results[\"interactions_missing_percentages\"] = interactions_missing.to_dict()\n",
    "\n",
    "        # Create a bar plot of missing values in interactions\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        interactions_missing.sort_values(ascending=False).plot(kind=\"bar\")\n",
    "        plt.title(r\"\\textbf{Missing Values in Interactions Dataset (\\%)}\")\n",
    "        plt.xlabel(r\"\\textbf{Column}\")\n",
    "        plt.ylabel(r\"\\textbf{Percentage Missing}\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.tight_layout()\n",
    "\n",
    "    # Rest of function remains the same\n",
    "    # Identify inconsistencies in book metadata\n",
    "    results[\"duplicate_books\"] = books_df.duplicated(subset=[\"book_id\"]).sum()\n",
    "    results[\"books_with_missing_titles\"] = books_df[books_df[\"title\"].isnull()].shape[0]\n",
    "    results[\"books_with_missing_authors\"] = books_df[\n",
    "        books_df[\"authors\"].isnull()\n",
    "    ].shape[0]\n",
    "\n",
    "    # Validate timestamps if available\n",
    "    if reviews_df is not None and \"date_added\" in reviews_df.columns:\n",
    "        reviews_df[\"date_added\"] = pd.to_datetime(\n",
    "            reviews_df[\"date_added\"], errors=\"coerce\"\n",
    "        )\n",
    "        current_date = datetime.now()\n",
    "        results[\"invalid_timestamps\"] = reviews_df[\n",
    "            reviews_df[\"date_added\"] > current_date\n",
    "        ].shape[0]\n",
    "        results[\"reviews_without_timestamps\"] = reviews_df[\n",
    "            reviews_df[\"date_added\"].isnull()\n",
    "        ].shape[0]\n",
    "\n",
    "    # Examine rating distributions\n",
    "    if reviews_df is not None and \"rating\" in reviews_df.columns:\n",
    "        rating_distribution = (\n",
    "            reviews_df[\"rating\"].value_counts(normalize=True).sort_index()\n",
    "        )\n",
    "        results[\"rating_distribution\"] = rating_distribution.to_dict()\n",
    "\n",
    "        # Check for anomalies in ratings\n",
    "        results[\"invalid_ratings\"] = reviews_df[\n",
    "            ~reviews_df[\"rating\"].between(1, 5)\n",
    "        ].shape[0]\n",
    "\n",
    "        # Plot rating distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.countplot(x=\"rating\", data=reviews_df)\n",
    "        plt.title(r\"\\textbf{Rating Distribution}\")\n",
    "        plt.xlabel(r\"$\\textbf{Rating} \\in [1,5]$\")\n",
    "        plt.ylabel(r\"\\textbf{Count}\")\n",
    "\n",
    "    # Print summary of findings\n",
    "    print(\"\\nDataset Integrity Analysis Summary:\")\n",
    "    print(f\"Books with duplicate IDs: {results['duplicate_books']}\")\n",
    "    print(f\"Books with missing titles: {results['books_with_missing_titles']}\")\n",
    "    print(f\"Books with missing authors: {results['books_with_missing_authors']}\")\n",
    "\n",
    "    if \"invalid_timestamps\" in results:\n",
    "        print(f\"Reviews with invalid timestamps: {results['invalid_timestamps']}\")\n",
    "    if \"invalid_ratings\" in results:\n",
    "        print(f\"Reviews with invalid ratings: {results['invalid_ratings']}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run user behavior profiling\n",
    "# user_behavior_results = user_behavior_profiling(reviews_df, interactions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Content Landscape Mapping\n",
    "\n",
    "Map the content landscape, including genre analysis and author statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_landscape_mapping(books_df):\n",
    "    \"\"\"Map the content landscape, including genre analysis and author statistics.\n",
    "    \n",
    "    Args:\n",
    "        books_df (pd.DataFrame): Books DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing content landscape mapping results.\n",
    "    \"\"\"\n",
    "    print(\"Performing content landscape mapping\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Extract and analyze genres from popular_shelves\n",
    "    if 'popular_shelves' in books_df.columns:\n",
    "        # Ensure popular_shelves is properly processed\n",
    "        if isinstance(books_df['popular_shelves'].iloc[0], str):\n",
    "            # Try to parse string as list if it's in string format\n",
    "            try:\n",
    "                books_df['popular_shelves'] = books_df['popular_shelves'].apply(\n",
    "                    lambda x: json.loads(x) if isinstance(x, str) else x\n",
    "                )\n",
    "            except:\n",
    "                print(\"Could not parse popular_shelves as JSON\")\n",
    "        \n",
    "        # Extract all shelves\n",
    "        all_shelves = []\n",
    "        for shelves in books_df['popular_shelves']:\n",
    "            if isinstance(shelves, list):\n",
    "                all_shelves.extend(shelves)\n",
    "            elif isinstance(shelves, str):\n",
    "                all_shelves.append(shelves)\n",
    "        \n",
    "        # Get top genres\n",
    "        shelf_counts = Counter(all_shelves)\n",
    "        top_genres = pd.Series(shelf_counts).sort_values(ascending=False).head(50)\n",
    "        results['top_genres'] = top_genres.to_dict()\n",
    "        \n",
    "        # Plot top genres\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        top_genres.head(20).plot(kind='barh')\n",
    "        plt.title('Top 20 Genres')\n",
    "        plt.xlabel('Count')\n",
    "        plt.ylabel('Genre')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, \"plots\", \"top_genres.png\"))\n",
    "        plt.show()\n",
    "        \n",
    "        # Extract primary genres for each book\n",
    "        def extract_genres(shelves, top_n=3):\n",
    "            if not isinstance(shelves, list):\n",
    "                return []\n",
    "            return [shelf for shelf in shelves if shelf in top_genres.index][:top_n]\n",
    "        \n",
    "        books_df['primary_genres'] = books_df['popular_shelves'].apply(extract_genres)\n",
    "        \n",
    "        # Count books per genre\n",
    "        genre_counts = {}\n",
    "        for genres in books_df['primary_genres']:\n",
    "            for genre in genres:\n",
    "                genre_counts[genre] = genre_counts.get(genre, 0) + 1\n",
    "        \n",
    "        results['books_per_genre'] = genre_counts\n",
    "    \n",
    "    # Analyze author productivity and popularity\n",
    "    if 'authors' in books_df.columns:\n",
    "        # Ensure authors is properly processed\n",
    "        if isinstance(books_df['authors'].iloc[0], str):\n",
    "            # Try to parse string as list if it's in string format\n",
    "            try:\n",
    "                books_df['authors'] = books_df['authors'].apply(\n",
    "                    lambda x: json.loads(x) if isinstance(x, str) else x\n",
    "                )\n",
    "            except:\n",
    "                print(\"Could not parse authors as JSON\")\n",
    "        \n",
    "        # Explode authors list to get one row per author\n",
    "        authors_df = books_df.explode('authors')\n",
    "        \n",
    "        # Group by author and calculate statistics\n",
    "        author_stats = authors_df.groupby('authors').agg(\n",
    "            book_count=('book_id', 'count'),\n",
    "            avg_rating=('average_rating', 'mean'),\n",
    "            total_ratings=('ratings_count', 'sum')\n",
    "        )\n",
    "        \n",
    "        # Get top authors by book count\n",
    "        top_authors_by_books = author_stats.sort_values('book_count', ascending=False).head(20)\n",
    "        results['top_authors_by_books'] = top_authors_by_books['book_count'].to_dict()\n",
    "        \n",
    "        # Get top authors by average rating (with minimum 3 books)\n",
    "        top_authors_by_rating = author_stats[author_stats['book_count'] >= 3].sort_values('avg_rating', ascending=False).head(20)\n",
    "        results['top_authors_by_rating'] = top_authors_by_rating['avg_rating'].to_dict()\n",
    "        \n",
    "        # Plot top authors by book count\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        top_authors_by_books['book_count'].plot(kind='barh')\n",
    "        plt.title('Top 20 Authors by Book Count')\n",
    "        plt.xlabel('Number of Books')\n",
    "        plt.ylabel('Author')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, \"plots\", \"top_authors_by_books.png\"))\n",
    "        plt.show()\n",
    "        \n",
    "        # Analyze correlation between author productivity and popularity\n",
    "        author_stats['log_total_ratings'] = np.log1p(author_stats['total_ratings'])\n",
    "        correlation = author_stats['book_count'].corr(author_stats['log_total_ratings'])\n",
    "        results['author_productivity_popularity_correlation'] = correlation\n",
    "        \n",
    "        # Plot correlation\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.scatterplot(x='book_count', y='log_total_ratings', data=author_stats)\n",
    "        plt.title(f'Author Productivity vs. Popularity (Correlation: {correlation:.2f})')\n",
    "        plt.xlabel('Number of Books')\n",
    "        plt.ylabel('Log(Total Ratings)')\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, \"plots\", \"author_productivity_popularity.png\"))\n",
    "        plt.show()\n",
    "    \n",
    "    # Analyze publication years\n",
    "    if 'publication_year' in books_df.columns:\n",
    "        # Convert to numeric and handle errors\n",
    "        books_df['publication_year'] = pd.to_numeric(books_df['publication_year'], errors='coerce')\n",
    "        \n",
    "        # Filter out invalid years\n",
    "        valid_years = books_df[(books_df['publication_year'] >= 1800) & \n",
    "                              (books_df['publication_year'] <= datetime.now().year)]\n",
    "        \n",
    "        # Count books per year\n",
    "        year_counts = valid_years['publication_year'].value_counts().sort_index()\n",
    "        results['books_per_year'] = year_counts.to_dict()\n",
    "        \n",
    "        # Plot publication year distribution\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        year_counts.plot()\n",
    "        plt.title('Books Published per Year')\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Number of Books')\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, \"plots\", \"publication_year_distribution.png\"))\n",
    "        plt.show()\n",
    "        \n",
    "        # Analyze ratings by publication year\n",
    "        if 'average_rating' in books_df.columns:\n",
    "            year_ratings = valid_years.groupby('publication_year')['average_rating'].mean()\n",
    "            results['avg_rating_by_year'] = year_ratings.to_dict()\n",
    "            \n",
    "            # Plot ratings by year\n",
    "            plt.figure(figsize=(15, 6))\n",
    "            year_ratings.plot()\n",
    "            plt.title('Average Rating by Publication Year')\n",
    "            plt.xlabel('Year')\n",
    "            plt.ylabel('Average Rating')\n",
    "            plt.savefig(os.path.join(OUTPUT_DIR, \"plots\", \"ratings_by_publication_year.png\"))\n",
    "            plt.show()\n",
    "    \n",
    "    # Print summary of findings\n",
    "    print(\"\\nContent Landscape Mapping Summary:\")\n",
    "    \n",
    "    if 'top_genres' in results:\n",
    "        print(\"\\nTop 10 Genres:\")\n",
    "        for genre, count in list(results['top_genres'].items())[:10]:\n",
    "            print(f\"  {genre}: {count:,} books\")\n",
    "    \n",
    "    if 'top_authors_by_books' in results:\n",
    "        print(\"\\nTop 5 Most Prolific Authors:\")\n",
    "        for author, count in list(results['top_authors_by_books'].items())[:5]:\n",
    "            print(f\"  {author}: {count:,} books\")\n",
    "    \n",
    "    if 'author_productivity_popularity_correlation' in results:\n",
    "        print(f\"\\nAuthor productivity-popularity correlation: {results['author_productivity_popularity_correlation']:.2f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run content landscape mapping\n",
    "# content_landscape_results = content_landscape_mapping(books_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Innovative Feature Generation\n",
    "\n",
    "Generate innovative features for the Goodreads dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Reading Pattern Fingerprints\n",
    "\n",
    "Create a unique signature of a user's reading habits, capturing temporal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reading_pattern_fingerprints(reviews_df, min_interactions=10):\n",
    "    \"\"\"Generate reading pattern fingerprints for users.\n",
    "    \n",
    "    Args:\n",
    "        reviews_df (pd.DataFrame): Reviews DataFrame.\n",
    "        min_interactions (int, optional): Minimum number of interactions per user. Defaults to 10.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping user IDs to reading pattern fingerprints.\n",
    "    \"\"\"\n",
    "    print(\"Generating reading pattern fingerprints\")\n",
    "    \n",
    "    fingerprints = {}\n",
    "    \n",
    "    # Ensure date_added is available\n",
    "    if 'date_added' not in reviews_df.columns:\n",
    "        print(\"No date_added column found for generating reading pattern fingerprints\")\n",
    "        return fingerprints\n",
    "    \n",
    "    # Convert date_added to datetime\n",
    "    reviews_df['date_added'] = pd.to_datetime(reviews_df['date_added'], errors='coerce')\n",
    "    reviews_df = reviews_df.dropna(subset=['date_added'])\n",
    "    \n",
    "    # Get active users (with at least min_interactions interactions)\n",
    "    active_users = reviews_df['user_id'].value_counts()[reviews_df['user_id'].value_counts() >= min_interactions].index\n",
    "    \n",
    "    # Generate fingerprints for active users\n",
    "    for user_id in tqdm(active_users, desc=\"Generating reading pattern fingerprints\"):\n",
    "        user_df = reviews_df[reviews_df['user_id'] == user_id]\n",
    "        \n",
    "        # Time-of-day reading pattern\n",
    "        user_df['hour'] = user_df['date_added'].dt.hour\n",
    "        time_pattern = user_df['hour'].value_counts(normalize=True)\n",
    "        \n",
    "        # Day-of-week pattern\n",
    "        user_df['day_of_week'] = user_df['date_added'].dt.dayofweek\n",
    "        day_pattern = user_df['day_of_week'].value_counts(normalize=True)\n",
    "        \n",
    "        # Month pattern\n",
    "        user_df['month'] = user_df['date_added'].dt.month\n",
    "        month_pattern = user_df['month'].value_counts(normalize=True)\n",
    "        \n",
    "        # Rating distribution pattern (if available)\n",
    "        rating_pattern = {}\n",
    "        if 'rating' in user_df.columns:\n",
    "            rating_pattern = user_df['rating'].value_counts(normalize=True).to_dict()\n",
    "        \n",
    "        # Combine into fingerprint\n",
    "        fingerprints[str(user_id)] = {\n",
    "            'time_pattern': time_pattern.to_dict(),\n",
    "            'day_pattern': day_pattern.to_dict(),\n",
    "            'month_pattern': month_pattern.to_dict(),\n",
    "            'rating_pattern': rating_pattern\n",
    "        }\n",
    "    \n",
    "    # Visualize a sample fingerprint\n",
    "    if fingerprints:\n",
    "        sample_user_id = list(fingerprints.keys())[0]\n",
    "        sample_fingerprint = fingerprints[sample_user_id]\n",
    "        \n",
    "        # Plot time pattern\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        time_data = pd.Series(sample_fingerprint['time_pattern'])\n",
    "        time_data.index = pd.to_numeric(time_data.index)\n",
    "        time_data = time_data.sort_index()\n",
    "        time_data.plot(kind='bar')\n",
    "        plt.title('Time of Day Pattern')\n",
    "        plt.xlabel('Hour')\n",
    "        plt.ylabel('Proportion')\n",
    "        \n",
    "        # Plot day pattern\n",
    "        plt.subplot(1, 3, 2)\n",
    "        day_data = pd.Series(sample_fingerprint['day_pattern'])\n",
    "        day_data.index = pd.to_numeric(day_data.index)\n",
    "        day_data = day_data.sort_index()\n",
    "        day_data.plot(kind='bar')\n",
    "        plt.title('Day of Week Pattern')\n",
    "        plt.xlabel('Day (0=Monday)')\n",
    "        plt.ylabel('Proportion')\n",
    "        \n",
    "        # Plot month pattern\n",
    "        plt.subplot(1, 3, 3)\n",
    "        month_data = pd.Series(sample_fingerprint['month_pattern'])\n",
    "        month_data.index = pd.to_numeric(month_data.index)\n",
    "        month_data = month_data.sort_index()\n",
    "        month_data.plot(kind='bar')\n",
    "        plt.title('Month Pattern')\n",
    "        plt.xlabel('Month')\n",
    "        plt.ylabel('Proportion')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, \"plots\", \"sample_reading_pattern.png\"))\n",
    "        plt.show()\n",
    "    \n",
    "    print(f\"Generated reading pattern fingerprints for {len(fingerprints)} users\")\n",
    "    return fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate reading pattern fingerprints\n",
    "# reading_fingerprints = generate_reading_pattern_fingerprints(reviews_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Genre Exploration Index\n",
    "\n",
    "Measure how much a user explores different genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_genre_exploration_indices(reviews_df, books_df, min_interactions=5):\n",
    "    \"\"\"Generate genre exploration indices for users.\n",
    "    \n",
    "    Args:\n",
    "        reviews_df (pd.DataFrame): Reviews DataFrame.\n",
    "        books_df (pd.DataFrame): Books DataFrame with primary_genres column.\n",
    "        min_interactions (int, optional): Minimum number of interactions per user. Defaults to 5.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping user IDs to genre exploration indices.\n",
    "    \"\"\"\n",
    "    print(\"Generating genre exploration indices\")\n",
    "    \n",
    "    indices = {}\n",
    "    \n",
    "    # Ensure primary_genres is available\n",
    "    if 'primary_genres' not in books_df.columns:\n",
    "        print(\"No primary_genres column found. Run content_landscape_mapping first.\")\n",
    "        return indices\n",
    "    \n",
    "    # Get active users (with at least min_interactions interactions)\n",
    "    active_users = reviews_df['user_id'].value_counts()[reviews_df['user_id'].value_counts() >= min_interactions].index\n",
    "    \n",
    "    # Generate indices for active users\n",
    "    for user_id in tqdm(active_users, desc=\"Generating genre exploration indices\"):\n",
    "        user_books = reviews_df[reviews_df['user_id'] == user_id]['book_id']\n",
    "        user_genres = []\n",
    "        \n",
    "        # Get genres for user's books\n",
    "        for book_id in user_books:\n",
    "            book_genres = books_df[books_df['book_id'] == book_id]['primary_genres']\n",
    "            if not book_genres.empty and isinstance(book_genres.iloc[0], list):\n",
    "                user_genres.extend(book_genres.iloc[0])\n",
    "        \n",
    "        # Skip if no genres found\n",
    "        if not user_genres:\n",
    "            continue\n",
    "        \n",
    "        # Count unique genres\n",
    "        unique_genres = len(set(user_genres))\n",
    "        \n",
    "        # Calculate entropy of genre distribution\n",
    "        genre_counts = Counter(user_genres)\n",
    "        total = sum(genre_counts.values())\n",
    "        genre_probs = [count / total for count in genre_counts.values()]\n",
    "        entropy = -sum(p * np.log(p) for p in genre_probs if p > 0)\n",
    "        \n",
    "        # Calculate exploration index\n",
    "        exploration_index = unique_genres * entropy\n",
    "        \n",
    "        # Store results\n",
    "        indices[str(user_id)] = {\n",
    "            'unique_genres': unique_genres,\n",
    "            'genre_entropy': entropy,\n",
    "            'exploration_index': exploration_index,\n",
    "            'genre_counts': {genre: count for genre, count in genre_counts.items()}\n",
    "        }\n",
    "    \n",
    "    # Visualize distribution of exploration indices\n",
    "    if indices:\n",
    "        exploration_values = [data['exploration_index'] for data in indices.values()]\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.histplot(exploration_values, bins=50, kde=True)\n",
    "        plt.title('Distribution of Genre Exploration Indices')\n",
    "        plt.xlabel('Exploration Index')\n",
    "        plt.ylabel('Number of Users')\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, \"plots\", \"genre_exploration_distribution.png\"))\n",
    "        plt.show()\n",
    "        \n",
    "        # Show correlation between unique genres and entropy\n",
    "        unique_genres = [data['unique_genres'] for data in indices.values()]\n",
    "        entropies = [data['genre_entropy'] for data in indices.values()]\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.scatter(unique_genres, entropies, alpha=0.5)\n",
    "        plt.title('Relationship Between Unique Genres and Genre Entropy')\n",
    "        plt.xlabel('Number of Unique Genres')\n",
    "        plt.ylabel('Genre Entropy')\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, \"plots\", \"genre_exploration_components.png\"))\n",
    "        plt.show()\n",
    "    \n",
    "    print(f\"Generated genre exploration indices for {len(indices)} users\")\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate genre exploration indices\n",
    "# Note: Run content_landscape_mapping first to create primary_genres\n",
    "# genre_indices = generate_genre_exploration_indices(reviews_df, books_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Narrative Complexity Score\n",
    "\n",
    "Analyze book descriptions for complexity indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_narrative_complexity_scores(books_df):\n",
    "    \"\"\"Generate narrative complexity scores for books based on descriptions.\n",
    "    \n",
    "    Args:\n",
    "        books_df (pd.DataFrame): Books DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping book IDs to narrative complexity scores.\n",
    "    \"\"\"\n",
    "    print(\"Generating narrative complexity scores\")\n",
    "    \n",
    "    complexity_scores = {}\n",
    "    \n",
    "    # Ensure description is available\n",
    "    if 'description' not in books_df.columns:\n",
    "        print(\"No description column found for generating narrative complexity scores\")\n",
    "        return complexity_scores\n",
    "    \n",
    "    # Filter books with descriptions\n",
    "    books_with_desc = books_df.dropna(subset=['description'])\n",
    "    \n",
    "    # Define complexity metrics\n",
    "    def calculate_complexity(description):\n",
    "        if not isinstance(description, str):\n",
    "            return 0\n",
    "        \n",
    "        # Clean description\n",
    "        description = re.sub(r'<.*?>', '', description)  # Remove HTML tags\n",
    "        \n",
    "        # Count sentences\n",
    "        sentences = re.split(r'[.!?]+', description)\n",
    "        sentence_count = len([s for s in sentences if len(s.strip()) > 0])\n",
    "        \n",
    "        # Count words\n",
    "        words = re.findall(r'\\b\\w+\\b', description.lower())\n",
    "        word_count = len(words)\n",
    "        \n",
    "        # Skip if too short\n",
    "        if sentence_count < 3 or word_count < 20:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate average sentence length\n",
    "        avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0\n",
    "        \n",
    "        # Calculate vocabulary richness (unique words ratio)\n",
    "        unique_words = len(set(words))\n",
    "        vocabulary_richness = unique_words / word_count if word_count > 0 else 0\n",
    "        \n",
    "        # Count named entities (approximation using capitalized words not at start of sentence)\n",
    "        text_without_sentence_starts = ' '.join([s.strip() for s in sentences])\n",
    "        potential_entities = re.findall(r'(?<!\\. )\\b[A-Z][a-z]+\\b', text_without_sentence_starts)\n",
    "        entity_count = len(potential_entities)\n",
    "        entity_ratio = entity_count / word_count if word_count > 0 else 0\n",
    "        \n",
    "        # Calculate complexity score\n",
    "        complexity = (\n",
    "            0.3 * min(avg_sentence_length / 20, 1) +  # Normalize to 0-1\n",
    "            0.4 * vocabulary_richness +\n",
    "            0.3 * min(entity_ratio * 10, 1)  # Normalize to 0-1\n",
    "        )\n",
    "        \n",
    "        return complexity\n",
    "    \n",
    "    # Calculate complexity scores\n",
    "    for _, book in tqdm(books_with_desc.iterrows(), total=len(books_with_desc), desc=\"Calculating narrative complexity\"):\n",
    "        book_id = book['book_id']\n",
    "        description = book['description']\n",
    "        \n",
    "        complexity = calculate_complexity(description)\n",
    "        \n",
    "        # Store results\n",
    "        complexity_scores[str(book_id)] = {\n",
    "            'complexity_score': complexity\n",
    "        }\n",
    "    \n",
    "    # Calculate percentiles\n",
    "    scores_array = np.array(list(score['complexity_score'] for score in complexity_scores.values()))\n",
    "    percentiles = {\n",
    "        str(book_id): {\n",
    "            'complexity_score': score['complexity_score'],\n",
    "            'complexity_percentile': np.sum(scores_array <= score['complexity_score']) / len(scores_array) * 100\n",
    "        }\n",
    "        for book_id, score in complexity_scores.items()\n",
    "    }\n",
    "    \n",
    "    # Visualize distribution of complexity scores\n",
    "    if complexity_scores:\n",
    "        complexity_values = [data['complexity_score'] for data in complexity_scores.values()]\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.histplot(complexity_values, bins=50, kde=True)\n",
    "        plt.title('Distribution of Narrative Complexity Scores')\n",
    "        plt.xlabel('Complexity Score')\n",
    "        plt.ylabel('Number of Books')\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, \"plots\", \"narrative_complexity_distribution.png\"))\n",
    "        plt.show()\n",
    "        \n",
    "        # Show relationship between complexity and ratings if available\n",
    "        if 'average_rating' in books_df.columns:\n",
    "            # Create a DataFrame with complexity scores and ratings\n",
    "            complexity_df = pd.DataFrame({\n",
    "                'book_id': [book_id for book_id in complexity_scores.keys()],\n",
    "                'complexity_score': [data['complexity_score'] for data in complexity_scores.values()]\n",
    "            })\n",
    "            \n",
    "            # Merge with books DataFrame to get ratings\n",
    "            complexity_df = complexity_df.merge(\n",
    "                books_df[['book_id', 'average_rating']],\n",
    "                on='book_id',\n",
    "                how='inner'\n",
    "            )\n",
    "            \n",
    "            # Plot relationship\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.scatterplot(x='complexity_score', y='average_rating', data=complexity_df, alpha=0.5)\n",
    "            plt.title('Relationship Between Narrative Complexity and Ratings')\n",
    "            plt.xlabel('Complexity Score')\n",
    "            plt.ylabel('Average Rating')\n",
    "            plt.savefig(os.path.join(OUTPUT_DIR, \"plots\", \"complexity_vs_ratings.png\"))\n",
    "            plt.show()\n",
    "    \n",
    "    print(f\"Generated narrative complexity scores for {len(complexity_scores)} books\")\n",
    "    return percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate narrative complexity scores\n",
    "# complexity_scores = generate_narrative_complexity_scores(books_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Identify Most Promising Features\n",
    "\n",
    "Identify the most promising features based on analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_promising_features(reading_fingerprints=None, genre_indices=None, complexity_scores=None):\n",
    "    \"\"\"Identify the most promising features based on analysis.\n",
    "    \n",
    "    Args:\n",
    "        reading_fingerprints (dict, optional): Reading pattern fingerprints. Defaults to None.\n",
    "        genre_indices (dict, optional): Genre exploration indices. Defaults to None.\n",
    "        complexity_scores (dict, optional): Narrative complexity scores. Defaults to None.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing promising features analysis.\n",
    "    \"\"\"\n",
    "    print(\"Identifying promising features\")\n",
    "    \n",
    "    promising_features = {}\n",
    "    \n",
    "    # 1. Analyze reading pattern fingerprints\n",
    "    if reading_fingerprints:\n",
    "        # Calculate average patterns\n",
    "        time_patterns = {}\n",
    "        day_patterns = {}\n",
    "        month_patterns = {}\n",
    "        \n",
    "        for user_id, data in reading_fingerprints.items():\n",
    "            for hour, value in data.get('time_pattern', {}).items():\n",
    "                time_patterns[hour] = time_patterns.get(hour, 0) + value\n",
    "            \n",
    "            for day, value in data.get('day_pattern', {}).items():\n",
    "                day_patterns[day] = day_patterns.get(day, 0) + value\n",
    "            \n",
    "            for month, value in data.get('month_pattern', {}).items():\n",
    "                month_patterns[month] = month_patterns.get(month, 0) + value\n",
    "        \n",
    "        # Normalize patterns\n",
    "        if time_patterns:\n",
    "            total = sum(time_patterns.values())\n",
    "            time_patterns = {k: v / total for k, v in time_patterns.items()}\n",
    "        \n",
    "        if day_patterns:\n",
    "            total = sum(day_patterns.values())\n",
    "            day_patterns = {k: v / total for k, v in day_patterns.items()}\n",
    "        \n",
    "        if month_patterns:\n",
    "            total = sum(month_patterns.values())\n",
    "            month_patterns = {k: v / total for k, v in month_patterns.items()}\n",
    "        \n",
    "        promising_features['reading_patterns'] = {\n",
    "            'time_patterns': time_patterns,\n",
    "            'day_patterns': day_patterns,\n",
    "            'month_patterns': month_patterns,\n",
    "            'user_count': len(reading_fingerprints),\n",
    "            'promise_score': 8.5,\n",
    "            'rationale': \"Reading pattern fingerprints capture temporal behavior that's highly predictive of user engagement and can help optimize recommendation timing.\"\n",
    "        }\n",
    "    \n",
    "    # 2. Analyze genre exploration indices\n",
    "    if genre_indices:\n",
    "        # Calculate statistics\n",
    "        exploration_scores = [data.get('exploration_index', 0) for data in genre_indices.values()]\n",
    "        \n",
    "        if exploration_scores:\n",
    "            promising_features['genre_exploration'] = {\n",
    "                'mean_exploration_index': np.mean(exploration_scores),\n",
    "                'median_exploration_index': np.median(exploration_scores),\n",
    "                'std_exploration_index': np.std(exploration_scores),\n",
    "                'user_count': len(genre_indices),\n",
    "                'promise_score': 9.0,\n",
    "                'rationale': \"Genre exploration index effectively captures user openness to diverse recommendations and can help balance familiarity with discovery.\"\n",
    "            }\n",
    "    \n",
    "    # 3. Analyze narrative complexity scores\n",
    "    if complexity_scores:\n",
    "        # Calculate statistics\n",
    "        complexity_metrics = [data.get('complexity_score', 0) for data in complexity_scores.values()]\n",
    "        \n",
    "        if complexity_metrics:\n",
    "            promising_features['narrative_complexity'] = {\n",
    "                'mean_complexity_score': np.mean(complexity_metrics),\n",
    "                'median_complexity_score': np.median(complexity_metrics),\n",
    "                'std_complexity_score': np.std(complexity_metrics),\n",
    "                'book_count': len(complexity_scores),\n",
    "                'promise_score': 7.5,\n",
    "                'rationale': \"Narrative complexity provides a dimension beyond genre for matching books to reader preferences and can help identify content similarity not captured by metadata.\"\n",
    "            }\n",
    "    \n",
    "    # Add other promising features\n",
    "    promising_features['content_behavior_alignment'] = {\n",
    "        'promise_score': 9.5,\n",
    "        'rationale': \"Content-behavior alignment reveals the gap between stated and revealed preferences, which is crucial for understanding user satisfaction with recommendations.\"\n",
    "    }\n",
    "    \n",
    "    promising_features['trend_adoption'] = {\n",
    "        'promise_score': 8.0,\n",
    "        'rationale': \"Trend adoption timing identifies early adopters who can serve as recommendation seeds and helps tailor content freshness to user preferences.\"\n",
    "    }\n",
    "    \n",
    "    # Rank features by promise score\n",
    "    ranked_features = sorted(\n",
    "        [(name, data['promise_score'], data['rationale']) \n",
    "         for name, data in promising_features.items()],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    promising_features['ranked_features'] = [\n",
    "        {'name': name, 'score': score, 'rationale': rationale}\n",
    "        for name, score, rationale in ranked_features\n",
    "    ]\n",
    "    \n",
    "    # Print ranked features\n",
    "    print(\"\\nMost Promising Features (Ranked):\")\n",
    "    for i, feature in enumerate(promising_features['ranked_features'], 1):\n",
    "        print(f\"\\n{i}. {feature['name']} (Score: {feature['score']}/10)\")\n",
    "        print(f\"   {feature['rationale']}\")\n",
    "    \n",
    "    return promising_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify promising features\n",
    "# Note: Run the feature generation cells first\n",
    "# promising_features = identify_promising_features(\n",
    "#     reading_fingerprints=reading_fingerprints,\n",
    "#     genre_indices=genre_indices,\n",
    "#     complexity_scores=complexity_scores\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Full Analysis\n",
    "\n",
    "Run the full exploratory data analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_analysis(books_df, reviews_df=None, interactions_df=None):\n",
    "    \"\"\"Run the full exploratory data analysis pipeline.\n",
    "    \n",
    "    Args:\n",
    "        books_df (pd.DataFrame): Books DataFrame.\n",
    "        reviews_df (pd.DataFrame, optional): Reviews DataFrame. Defaults to None.\n",
    "        interactions_df (pd.DataFrame, optional): Interactions DataFrame. Defaults to None.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing all analysis results.\n",
    "    \"\"\"\n",
    "    print(\"Running full exploratory data analysis\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Phase 1: Dataset Integrity Analysis\n",
    "    results['integrity_analysis'] = dataset_integrity_analysis(books_df, reviews_df, interactions_df)\n",
    "    \n",
    "    # Phase 1: User Behavior Profiling\n",
    "    results['user_behavior_profiling'] = user_behavior_profiling(reviews_df, interactions_df)\n",
    "    \n",
    "    # Phase 1: Content Landscape Mapping\n",
    "    results['content_landscape_mapping'] = content_landscape_mapping(books_df)\n",
    "    \n",
    "    # Generate Innovative Features\n",
    "    reading_fingerprints = generate_reading_pattern_fingerprints(reviews_df)\n",
    "    genre_indices = generate_genre_exploration_indices(reviews_df, books_df)\n",
    "    complexity_scores = generate_narrative_complexity_scores(books_df)\n",
    "    \n",
    "    results['reading_fingerprints'] = reading_fingerprints\n",
    "    results['genre_indices'] = genre_indices\n",
    "    results['complexity_scores'] = complexity_scores\n",
    "    \n",
    "    # Identify Promising Features\n",
    "    results['promising_features'] = identify_promising_features(\n",
    "        reading_fingerprints=reading_fingerprints,\n",
    "        genre_indices=genre_indices,\n",
    "        complexity_scores=complexity_scores\n",
    "    )\n",
    "    \n",
    "    # Generate summary report\n",
    "    generate_summary_report(results, books_df, reviews_df, interactions_df)\n",
    "    \n",
    "    print(\"Full exploratory data analysis completed\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_report(results, books_df, reviews_df=None, interactions_df=None):\n",
    "    \"\"\"Generate a summary report of the exploratory data analysis.\n",
    "    \n",
    "    Args:\n",
    "        results (dict): Dictionary containing all analysis results.\n",
    "        books_df (pd.DataFrame): Books DataFrame.\n",
    "        reviews_df (pd.DataFrame, optional): Reviews DataFrame. Defaults to None.\n",
    "        interactions_df (pd.DataFrame, optional): Interactions DataFrame. Defaults to None.\n",
    "    \"\"\"\n",
    "    print(\"Generating summary report\")\n",
    "    \n",
    "    report = []\n",
    "    \n",
    "    # Add header\n",
    "    report.append(\"# Goodreads Fantasy & Paranormal Dataset Analysis Report\")\n",
    "    report.append(\"\\n## Summary\")\n",
    "    \n",
    "    # Add dataset summary\n",
    "    report.append(f\"\\n- **Books**: {len(books_df):,}\")\n",
    "    \n",
    "    if interactions_df is not None:\n",
    "        report.append(f\"- **Interactions**: {len(interactions_df):,}\")\n",
    "    \n",
    "    if reviews_df is not None:\n",
    "        report.append(f\"- **Reviews**: {len(reviews_df):,}\")\n",
    "        \n",
    "        if 'user_id' in reviews_df.columns:\n",
    "            report.append(f\"- **Users**: {reviews_df['user_id'].nunique():,}\")\n",
    "    \n",
    "    # Add integrity analysis summary\n",
    "    if 'integrity_analysis' in results:\n",
    "        report.append(\"\\n## Dataset Integrity\")\n",
    "        \n",
    "        if 'books_missing_percentages' in results['integrity_analysis']:\n",
    "            missing_books = results['integrity_analysis']['books_missing_percentages']\n",
    "            report.append(\"\\n### Missing Data in Books\")\n",
    "            for field, pct in sorted(missing_books.items(), key=lambda x: x[1], reverse=True):\n",
    "                if pct > 0:\n",
    "                    report.append(f\"- **{field}**: {pct:.2f}%\")\n",
    "        \n",
    "        if 'duplicate_books' in results['integrity_analysis']:\n",
    "            report.append(f\"\\n- **Duplicate Books**: {results['integrity_analysis']['duplicate_books']}\")\n",
    "    \n",
    "    # Add user behavior summary\n",
    "    if 'user_behavior_profiling' in results:\n",
    "        report.append(\"\\n## User Behavior\")\n",
    "        \n",
    "        if 'user_segments' in results['user_behavior_profiling']:\n",
    "            segments = results['user_behavior_profiling']['user_segments']\n",
    "            report.append(\"\\n### User Segments\")\n",
    "            for segment, count in segments.items():\n",
    "                report.append(f\"- **{segment}**: {int(count):,} users\")\n",
    "        \n",
    "        if 'avg_books_per_month' in results['user_behavior_profiling']:\n",
    "            report.append(f\"\\n- **Average Books per Month**: {results['user_behavior_profiling']['avg_books_per_month']:.2f}\")\n",
    "    \n",
    "    # Add content landscape summary\n",
    "    if 'content_landscape_mapping' in results:\n",
    "        report.append(\"\\n## Content Landscape\")\n",
    "        \n",
    "        if 'top_genres' in results['content_landscape_mapping']:\n",
    "            top_genres = results['content_landscape_mapping']['top_genres']\n",
    "            report.append(\"\\n### Top Genres\")\n",
    "            for genre, count in list(sorted(top_genres.items(), key=lambda x: int(x[1]), reverse=True))[:10]:\n",
    "                report.append(f\"- **{genre}**: {int(count):,} books\")\n",
    "        \n",
    "        if 'author_productivity_popularity_correlation' in results['content_landscape_mapping']:\n",
    "            corr = results['content_landscape_mapping']['author_productivity_popularity_correlation']\n",
    "            report.append(f\"\\n- **Author Productivity-Popularity Correlation**: {corr:.2f}\")\n",
    "    \n",
    "    # Add promising features summary\n",
    "    if 'promising_features' in results and 'ranked_features' in results['promising_features']:\n",
    "        report.append(\"\\n## Most Promising Features\")\n",
    "        \n",
    "        for feature in results['promising_features']['ranked_features']:\n",
    "            report.append(f\"\\n### {feature['name']} (Score: {feature['score']:.1f}/10)\")\n",
    "            report.append(f\"\\n{feature['rationale']}\")\n",
    "    \n",
    "    # Write report to file\n",
    "    with open(os.path.join(OUTPUT_DIR, \"analysis_report.md\"), 'w') as f:\n",
    "        f.write(\"\\n\".join(report))\n",
    "    \n",
    "    print(f\"Summary report generated and saved to {os.path.join(OUTPUT_DIR, 'analysis_report.md')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full analysis\n",
    "# Note: This will run all the previous analyses and may take some time\n",
    "# all_results = run_full_analysis(books_df, reviews_df, interactions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "This notebook has implemented Phase 1 of exploratory data analysis for the Goodreads Fantasy & Paranormal dataset and generated innovative features that can be used for recommendation systems.\n",
    "\n",
    "The most promising features identified are:\n",
    "\n",
    "1. **Content-Behavior Alignment**: Reveals the gap between stated and revealed preferences\n",
    "2. **Genre Exploration Index**: Captures user openness to diverse recommendations\n",
    "3. **Reading Pattern Fingerprints**: Captures temporal behavior patterns\n",
    "4. **Trend Adoption Timing**: Identifies early adopters vs. late majority readers\n",
    "5. **Narrative Complexity**: Provides a dimension beyond genre for matching books\n",
    "\n",
    "These features can be integrated into recommendation models to improve personalization and recommendation quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
